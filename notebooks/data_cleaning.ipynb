{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d42971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP & DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "calendar_df = pd.read_csv('../data/calendar.csv.gz', compression='gzip').sample(frac=0.1, random_state=42)\n",
    "reviews = pd.read_csv('../data/reviews.csv.gz', compression='gzip').sample(frac=0.1, random_state=42)\n",
    "listings = pd.read_csv('../data/listings.csv.gz', compression='gzip').sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1bic63pdk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALENDAR DATASET\n",
    "# ============================================================\n",
    "# 1: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e91c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset preview and missing value analysis\n",
    "display(calendar_df.head(20))\n",
    "print(\"MISSING VALUES OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_count = calendar_df.isnull().sum()\n",
    "missing_percent = (calendar_df.isnull().sum() / len(calendar_df)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_count,\n",
    "    'Missing_Percent': missing_percent\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop price columns (100% missing, will merge from listings later)\n",
    "calendar_df = calendar_df.drop(['price', 'adjusted_price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ztmo1phg73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CALENDAR DATASET\n",
    "# ============================================================\n",
    "# 2: DATA CLEANING & PREPROCESSING\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime and extract temporal features\n",
    "calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
    "calendar_df['year'] = calendar_df['date'].dt.year\n",
    "calendar_df['month'] = calendar_df['date'].dt.month\n",
    "calendar_df['day_of_week'] = calendar_df['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "calendar_df['week_of_year'] = calendar_df['date'].dt.isocalendar().week\n",
    "calendar_df['is_weekend'] = calendar_df['day_of_week'].isin([5, 6]).astype(bool)\n",
    "\n",
    "# Convert 'available' to boolean\n",
    "calendar_df['available'] = calendar_df['available'].map({'t': True, 'f': False})\n",
    "\n",
    "# Set appropriate data types\n",
    "calendar_df['listing_id'] = calendar_df['listing_id'].astype('int64')\n",
    "calendar_df['minimum_nights'] = calendar_df['minimum_nights'].astype('int16')\n",
    "calendar_df['maximum_nights'] = calendar_df['maximum_nights'].astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ns1edw19exh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data integrity and display cleaned calendar dataset\n",
    "print(\"Calendar Dataset Info:\")\n",
    "print(f\"Shape: {calendar_df.shape}\")\n",
    "print(f\"Any missing values: {calendar_df.isnull().sum().sum()}\")\n",
    "print(\"\\nData types:\")\n",
    "print(calendar_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g69ziaxhela",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Duplicate Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total rows: {len(calendar_df)}\")\n",
    "print(f\"Duplicate rows (all columns): {calendar_df.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows (listing_id, date): {calendar_df.duplicated(subset=['listing_id', 'date']).sum()}\")\n",
    "\n",
    "if calendar_df.duplicated(subset=['listing_id', 'date']).sum() > 0:\n",
    "    print(\"\\nRemoving duplicates on (listing_id, date)...\")\n",
    "    calendar_df = calendar_df.drop_duplicates(subset=['listing_id', 'date'], keep='first')\n",
    "    print(f\"New shape after removing duplicates: {calendar_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates found on (listing_id, date) - dataset is clean!\")\n",
    "\n",
    "#No duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "locvz692r6i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for extreme values and outliers\n",
    "print(\"Extreme Values Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Numeric columns to analyze\n",
    "numeric_cols = ['minimum_nights', 'maximum_nights', 'month', 'day_of_week', 'week_of_year']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Min: {calendar_df[col].min()}\")\n",
    "    print(f\"  Max: {calendar_df[col].max()}\")\n",
    "    print(f\"  Mean: {calendar_df[col].mean():.2f}\")\n",
    "    print(f\"  Median: {calendar_df[col].median():.2f}\")\n",
    "    print(f\"  Std: {calendar_df[col].std():.2f}\")\n",
    "\n",
    "# Check for unrealistic night values\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Unrealistic Values Check:\")\n",
    "print(f\"minimum_nights > 365: {(calendar_df['minimum_nights'] > 365).sum()} rows\")\n",
    "print(f\"maximum_nights > 1000: {(calendar_df['maximum_nights'] > 1000).sum()} rows\")\n",
    "print(f\"minimum_nights > maximum_nights: {(calendar_df['minimum_nights'] > calendar_df['maximum_nights']).sum()} rows\")\n",
    "\n",
    "# Show distribution of extreme values\n",
    "if (calendar_df['maximum_nights'] > 1000).sum() > 0:\n",
    "    print(f\"\\nExamples of maximum_nights > 1000:\")\n",
    "    print(calendar_df[calendar_df['maximum_nights'] > 1000][['listing_id', 'minimum_nights', 'maximum_nights']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q0ip5q7z54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CALENDAR DATASET\n",
    "# ============================================================\n",
    "# 3: DATA QUALITY CHECKS & VALIDATION\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "han3vb7uq0i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix negative maximum_nights values with placeholder\n",
    "print(\"Fixing Negative Values:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Rows with negative maximum_nights: {(calendar_df['maximum_nights'] < 0).sum()}\")\n",
    "\n",
    "if (calendar_df['maximum_nights'] < 0).sum() > 0:\n",
    "    print(\"Replacing negative maximum_nights with 365 (1 year placeholder)...\")\n",
    "    calendar_df.loc[calendar_df['maximum_nights'] < 0, 'maximum_nights'] = 365\n",
    "    print(f\"Fixed! New max value: {calendar_df['maximum_nights'].max()}\")\n",
    "\n",
    "print(f\"\\nRows with negative minimum_nights: {(calendar_df['minimum_nights'] < 0).sum()}\")\n",
    "if (calendar_df['minimum_nights'] < 0).sum() > 0:\n",
    "    print(\"Replacing negative minimum_nights with 1 (minimum stay)...\")\n",
    "    calendar_df.loc[calendar_df['minimum_nights'] < 0, 'minimum_nights'] = 1\n",
    "    print(f\"Fixed!\")\n",
    "\n",
    "# Verify fix\n",
    "print(f\"\\nVerification - minimum_nights range: [{calendar_df['minimum_nights'].min()}, {calendar_df['minimum_nights'].max()}]\")\n",
    "print(f\"Verification - maximum_nights range: [{calendar_df['maximum_nights'].min()}, {calendar_df['maximum_nights'].max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28133144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DETAILED INVESTIGATION OF PROBLEMS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check negative maximum_nights\n",
    "negative_max = calendar_df[calendar_df['maximum_nights'] < 0]\n",
    "print(f\"\\n1. NEGATIVE MAXIMUM_NIGHTS: {len(negative_max)} rows\")\n",
    "if len(negative_max) > 0:\n",
    "    print(negative_max[['listing_id', 'date', 'minimum_nights', 'maximum_nights']].head(20))\n",
    "    print(f\"\\nUnique listings affected: {negative_max['listing_id'].nunique()}\")\n",
    "\n",
    "# 2. Check extremely high maximum_nights (>2 years)\n",
    "extreme_max = calendar_df[calendar_df['maximum_nights'] > 730]\n",
    "print(f\"\\n2. EXTREMELY HIGH MAXIMUM_NIGHTS (>730 days): {len(extreme_max)} rows\")\n",
    "print(extreme_max['maximum_nights'].describe())\n",
    "print(f\"Unique listings: {extreme_max['listing_id'].nunique()}\")\n",
    "\n",
    "# 3. Check min > max (logical error)\n",
    "logical_error = calendar_df[calendar_df['minimum_nights'] > calendar_df['maximum_nights']]\n",
    "print(f\"\\n3. MIN > MAX (LOGICAL ERROR): {len(logical_error)} rows\")\n",
    "if len(logical_error) > 0:\n",
    "    print(logical_error[['listing_id', 'minimum_nights', 'maximum_nights']].head(10))\n",
    "\n",
    "# 4. Check very high minimum_nights\n",
    "high_min = calendar_df[calendar_df['minimum_nights'] > 365]\n",
    "print(f\"\\n4. MINIMUM_NIGHTS > 365 days: {len(high_min)} rows\")\n",
    "if len(high_min) > 0:\n",
    "    print(high_min[['listing_id', 'minimum_nights', 'maximum_nights']].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CALENDAR DATASET\n",
    "# ============================================================\n",
    "# 4: CREATING CLEANED DATA FILE\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bl0itnfofhb",
   "metadata": {},
   "outputs": [],
   "source": "# Export cleaned calendar dataset to CSV (compressed)\ncalendar_df.to_csv('../data/processed/calendar_cleaned.csv.gz', index=False, compression='gzip')\nprint(\"✓ Cleaned calendar dataset exported to: data/processed/calendar_cleaned.csv.gz\")\nprint(f\"  Shape: {calendar_df.shape}\")\nprint(f\"  Rows: {len(calendar_df):,} | Columns: {len(calendar_df.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81943d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REVIEWS DATASET\n",
    "# ============================================================\n",
    "# # 1: DATA CLEANING & PREPROCESSING\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we check if there are missing values or duplicates in the reviews dataset\n",
    "reviews.isna().sum()\n",
    "reviews.duplicated().sum()\n",
    "\n",
    "#We just drop the rows with missing values as there are none with duplicates\n",
    "reviews_clean = reviews.dropna()\n",
    "\n",
    "#We check again if there are missing values in the cleaned dataset\n",
    "reviews_clean.isna().sum()\n",
    "\n",
    "#Now we importying the TfidfVectorizer to transform the text data into numerical data, making it suitable for machine learning algorithms.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#We create the TF-IDF vectorizer with specific parameters, limiting the number of features to 50 in order to optimise computational time.\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', \n",
    "    ngram_range=(1, 2),  \n",
    "    max_features=50)\n",
    "\n",
    "#We fit and transform the 'comments' column from the cleaned reviews dataset as this is the only text column of the dataset.\n",
    "texts = reviews_clean['comments'].astype(str)\n",
    "reviews_final = vectorizer.fit_transform(texts)\n",
    "\n",
    "#We convert the resulting sparse matrix into a DataFrame for easier analysis and visualization.\n",
    "tfidf_df = pd.DataFrame(\n",
    "    reviews_final.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "#We check the first few rows of the TF-IDF DataFrame to see if the tf-idf transformation was successful.\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REVIEWS DATASET\n",
    "# ============================================================\n",
    "# 1: CREATING CLEANED DATA FILE\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa351ff",
   "metadata": {},
   "outputs": [],
   "source": "# Export cleaned reviews dataset to CSV (compressed)\nreviews_clean.to_csv('../data/processed/reviews_cleaned.csv.gz', index=False, compression='gzip')\nprint(\"✓ Cleaned reviews dataset exported to: data/processed/reviews_cleaned.csv.gz\")\nprint(f\"  Shape: {reviews_clean.shape}\")\nprint(f\"  Rows: {len(reviews_clean):,} | Columns: {len(reviews_clean.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LISTINGS DATASET\n",
    "# ============================================================\n",
    "# \n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== LISTINGS OVERVIEW ===\")\n",
    "print(f\"Shape: {listings.shape}\")\n",
    "print(f\"\\nColumns: {listings.shape[1]}\")\n",
    "print(listings.columns.tolist())\n",
    "print(f\"\\nData types:\\n{listings.dtypes}\")\n",
    "print(f\"\\nMissing values (%):\\n{(listings.isnull().sum() / len(listings) * 100).round(2)}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "listings.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types for targeted cleaning\n",
    "print(\"=== COLUMN ANALYSIS ===\\n\")\n",
    "\n",
    "# Numeric columns\n",
    "numeric_cols = listings.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns ({len(numeric_cols)}):\\n{numeric_cols}\\n\")\n",
    "\n",
    "# Text columns\n",
    "text_cols = listings.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Text columns ({len(text_cols)}):\\n{text_cols}\\n\")\n",
    "\n",
    "# Check sample values from text columns\n",
    "print(\"=== TEXT COLUMN SAMPLES ===\\n\")\n",
    "for col in text_cols[:10]:  # First 10 text columns\n",
    "    sample = listings[col].dropna().head(2).tolist()\n",
    "    print(f\"{col}: {sample}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types for targeted cleaning\n",
    "print(\"=== COLUMN ANALYSIS ===\\n\")\n",
    "\n",
    "# Numeric columns\n",
    "numeric_cols = listings.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\" Numeric columns ({len(numeric_cols)}):\")\n",
    "print(numeric_cols)\n",
    "\n",
    "# Text columns\n",
    "text_cols = listings.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\n Text columns ({len(text_cols)}):\")\n",
    "for col in text_cols:\n",
    "    sample = listings[col].dropna().iloc[0] if len(listings[col].dropna()) > 0 else \"N/A\"\n",
    "    print(f\"  {col}: {str(sample)[:60]}\")\n",
    "\n",
    "# Boolean columns\n",
    "print(f\"\\n Sample values to detect types:\")\n",
    "for col in text_cols[:5]:\n",
    "    sample = listings[col].dropna().head(2).tolist()\n",
    "    print(f\"  {col}: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 3: COMPREHENSIVE DATA CLEANING =====\n",
    "\n",
    "print(\"=== STARTING DATA CLEANING ===\\n\")\n",
    "\n",
    "# 1. CLEAN PERCENTAGE COLUMNS\n",
    "print(\"  CLEANING PERCENTAGE COLUMNS\")\n",
    "percentage_cols = [col for col in listings.columns if 'rate' in col.lower() or 'percent' in col.lower()]\n",
    "print(f\"   Found: {percentage_cols}\")\n",
    "\n",
    "for col in percentage_cols:\n",
    "    if col in listings.columns and listings[col].dtype == 'object':\n",
    "        listings[col] = listings[col].str.replace('%', '', regex=False).str.strip()\n",
    "        listings[col] = pd.to_numeric(listings[col], errors='coerce') / 100\n",
    "        print(f\"   ✓ {col} → converted to decimal\")\n",
    "\n",
    "# 2. CLEAN BOOLEAN COLUMNS (t/f to True/False)\n",
    "print(\"\\n  CLEANING BOOLEAN COLUMNS\")\n",
    "boolean_map = {'t': True, 'f': False, 'T': True, 'F': False}\n",
    "\n",
    "for col in text_cols:\n",
    "    if col in listings.columns:\n",
    "        unique_vals = listings[col].dropna().unique()\n",
    "        if len(unique_vals) <= 2 and any(v in boolean_map for v in unique_vals):\n",
    "            listings[col] = listings[col].map(boolean_map)\n",
    "            listings[col] = listings[col].fillna(False)\n",
    "            print(f\"   ✓ {col} → converted to boolean\")\n",
    "\n",
    "# 3. CLEAN PRICE COLUMNS\n",
    "print(\"\\n  CLEANING PRICE COLUMNS\")\n",
    "price_cols = [col for col in listings.columns if 'price' in col.lower()]\n",
    "for col in price_cols:\n",
    "    if col in listings.columns and listings[col].dtype == 'object':\n",
    "        listings[col] = listings[col].str.replace('$', '', regex=False)\n",
    "        listings[col] = listings[col].str.replace(',', '', regex=False)\n",
    "        listings[col] = pd.to_numeric(listings[col], errors='coerce')\n",
    "        print(f\"   ✓ {col} → cleaned & converted to numeric\")\n",
    "\n",
    "# 4. CLEAN NUMERIC COLUMNS - FILL MISSING\n",
    "print(\"\\n  FILLING MISSING NUMERIC VALUES\")\n",
    "numeric_cols = listings.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    missing_count = listings[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        listings[col].fillna(listings[col].median(), inplace=True)\n",
    "        print(f\"   ✓ {col}: filled {missing_count} missing values with median\")\n",
    "\n",
    "# 5. CLEAN TEXT COLUMNS - FILL MISSING\n",
    "print(\"\\n  FILLING MISSING TEXT VALUES\")\n",
    "text_cols = listings.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in text_cols:\n",
    "    missing_count = listings[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        mode_val = listings[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            listings[col].fillna(mode_val[0], inplace=True)\n",
    "        else:\n",
    "            listings[col].fillna('Unknown', inplace=True)\n",
    "        print(f\"   ✓ {col}: filled {missing_count} missing values\")\n",
    "\n",
    "# 6. CONVERT DATE COLUMNS\n",
    "print(\"\\n  CONVERTING DATE COLUMNS\")\n",
    "date_patterns = ['date', 'since', 'review', 'last']\n",
    "for col in listings.columns:\n",
    "    if any(pattern in col.lower() for pattern in date_patterns):\n",
    "        if listings[col].dtype == 'object':\n",
    "            listings[col] = pd.to_datetime(listings[col], errors='coerce')\n",
    "            print(f\"   ✓ {col} → converted to datetime\")\n",
    "\n",
    "# 7. DETECT & HANDLE OUTLIERS\n",
    "print(\"\\n  DETECTING OUTLIERS\")\n",
    "numeric_cols = listings.select_dtypes(include=[np.number]).columns.tolist()\n",
    "outlier_cols = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = listings[col].quantile(0.25)\n",
    "    Q3 = listings[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = ((listings[col] < lower_bound) | (listings[col] > upper_bound)).sum()\n",
    "    if outliers > 0:\n",
    "        # CAP instead of remove\n",
    "        listings[col] = listings[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        outlier_cols.append((col, outliers, lower_bound, upper_bound))\n",
    "        print(f\"   ✓ {col}: capped {outliers} outliers [{lower_bound:.2f} - {upper_bound:.2f}]\")\n",
    "\n",
    "# 8. FINAL VALIDATION\n",
    "print(\"\\n  FINAL VALIDATION\")\n",
    "print(f\"   Shape: {listings.shape}\")\n",
    "print(f\"   Missing values: {listings.isnull().sum().sum()}\")\n",
    "print(f\"   Duplicates: {listings.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n DATA CLEANING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LISTINGS DATASET\n",
    "# ============================================================\n",
    "# 4: CREATING CLEANED DATA FILE\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615643e7",
   "metadata": {},
   "outputs": [],
   "source": "# Export cleaned listings dataset to CSV (compressed)\nlistings.to_csv('../data/processed/listings_cleaned.csv.gz', index=False, compression='gzip')\nprint(\"✓ Cleaned listings dataset exported to: data/processed/listings_cleaned.csv.gz\")\nprint(f\"  Shape: {listings.shape}\")\nprint(f\"  Rows: {len(listings):,} | Columns: {len(listings.columns)}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}